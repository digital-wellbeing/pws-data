---
title: "Merge datasets"
output: html_document
date: "2022-09-06"
---

```{r setup, include=FALSE}
library(janitor)
library(knitr)
library(lubridate)
library(tidyverse)
```

This is all a work in progress
The goal is to create timeseries of all events in a session, and then merge the survey responses to the survey events

# Qualtrics

Clean qualtrics data.

```{r}
dq <- read_csv(
  "data-raw/qualtrics/Dataset_Live_1_September+9,+2022_16.27.csv", 
  col_types = cols_only(
    eventId = col_character(),
    entityId = col_character(),
    timeStamp = col_character(),
    age = col_integer(),
    gender = col_character(),
    promptType = col_character(),
    promptCategory = col_character(),
    response = col_double()  # Throws warnings for 'none' responses
    )
)
# Verify that only problem is response='none'
problems() %>% 
  distinct(col, expected, actual)

# Read timestamps as strings, otherwise it mysteriously errs on many rows
# convert to datetimes here, mysteriously does not err
dq <- dq %>% 
  mutate(timeStamp = as_datetime(timeStamp))

# There are data without a timestamp -- these are faulty responses between game and APIs
dq <- dq %>% 
  drop_na(timeStamp)

# Remove test data from qualtrics table
dq <- dq %>% 
  # Remove all data before launch
  # [todo] this removes data that happened in the year 1444
  filter(timeStamp >= ymd_hms("2022-08-18 14:00:00")) %>% 
  # Remove developer accounts
  filter(
    !(entityId %in% c(
      "1b4893d8239b65417ea746c92db9c1adc686e8a1", 
      "47d03dc7c44a9e0a39ba3b5de3650b8f6e41a1d8", 
      "3086fcd6630a9c61d84884d56e49d34f87127b7d", 
      "785f413cb080eeac412538dc6424f240feb5b239", 
      "cc0458b60500702da2faf44e8af870795d7d9b52"))
  )

# Fill demographics to each row and delete 
dq <- dq %>% 
  group_by(entityId) %>% 
  fill(age, gender) %>% 
  ungroup() %>% 
  filter(promptType != "age_gender_submitted")

# Rename and select relevant variables
dq <- dq %>% 
  select(
    pid = entityId,  # Let's call it pid for participant or player ID
    eid = eventId,  # eid for event ID
    age, gender, 
    timestamp = timeStamp, 
    event_name = promptType,
    response_type = promptCategory,
    response
  )

dq <- dq %>% 
  arrange(pid, timestamp)
```

todo at least for now we remove people's data whose local time was wildly incorrect

```{r}
dq <- dq %>% 
  filter(between(year(timestamp), 2022, 2023))
```

# PlayFab

```{r}
# Slow. Saves processed files by event name.
if (length(list.files("data-raw/playfab-processed/", pattern = "event-")) == 0) {
  dp <- read_csv("data-raw/playfab-processed/export.csv.gz")
  
  # First remove some unnecessary data
  
  # We only care about player events
  dp <- dp %>% 
    filter(EntityType == "player") %>% 
    select(-EntityType)
  
  # Completely remove data from players who requested data clearing
  remove_ids <- dp %>% 
    filter(EventName == "clear_data_requested") %>% 
    distinct(EntityId) %>% 
    pull(EntityId)
  dp <- dp %>% 
    filter(!(EntityId %in% remove_ids))
  
  # In an early version of PWS there was a bug that saved the game too often
  # here we remove save game rows that occurred within a second of the next one
  # Work temporarily with game_saved events separately
  dp_save <- dp %>% 
    filter(EventName == "game_saved")
  dp <- dp %>% 
    filter(EventName != "game_saved")
  dp_save <- dp_save %>%
    arrange(EntityId, Timestamp) %>% 
    mutate(
      from_next = lead(Timestamp) - Timestamp,
      # Don't delete rows where next ID is different
      from_next = if_else(lead(EntityId) == EntityId, from_next, 1000)
    ) %>% 
    filter(from_next > 1) %>% 
    select(-from_next)
  # Then rejoin tables again
  dp <- bind_rows(
    dp, dp_save
  ) %>% 
    arrange(EntityId, Timestamp)
  rm(dp_save)
  
  # PlayFab data has an EntityId that is unique to each player,
  # but that doesn't link to the qualtrics data. Instead we use OxfordStudyEntityId,
  # which doesn't exist on all rows. 
  # So we "complete" the OxfordStudyEntityId for each EntityId.
  dp <- dp %>% 
    group_by(EntityId) %>% 
    fill(OxfordStudyEntityId, .direction = "updown") %>% 
    ungroup()
  
  # There are a few rows where no match was found but we confirm that data is useless
  # dp %>% 
  #   filter(is.na(OxfordStudyEntityId)) %>% 
  #   View
  
  # We then replace the names and drop missing rows
  dp <- dp %>% 
    drop_na(OxfordStudyEntityId) %>% 
    select(-EntityId) %>% 
    rename(EntityId = OxfordStudyEntityId)
  
  # Arrange data
  dp <- dp %>% 
    arrange(EntityId, Timestamp)
  
  # Harmonise variable names
  dp <- dp %>% 
    clean_names() %>% 
    rename(
      pid = entity_id
    )
  
  # Save by event type
  dp %>% 
    group_by(event_name) %>% 
    group_walk(
      ~saveRDS(
        .x, 
        str_glue("data-raw/playfab-processed/event-{.y$event_name}.rds")
        ),
      .keep = TRUE
    )
}
```

# Merge

We define a participant as someone who answered at least one survey question, and had at least some telemetry. So we first remove participants that only exist in one dataset.

```{r}
# load relevant playfab event data
events <- c("mood_reported", "study_prompt_answered", "study_prompt_skipped")
dp <- str_glue("data-raw/playfab-processed/event-{events}.rds") %>% 
  map(readRDS) %>% 
  bind_rows() %>% 
  arrange(pid, timestamp)

# Playfab study_prompt_skipped translates to response=na in qualtrics
dp <- dp %>%
  mutate(
    event_name = if_else(
      event_name == "study_prompt_skipped",
      "study_prompt_answered",
      event_name
    )
  )

pids <- inner_join(
  distinct(dq, pid),
  distinct(dp, pid)
) %>% 
  pull(pid)
dq <- filter(dq, pid %in% pids)
dp <- filter(dp, pid %in% pids)
```

TODO:

Before 2022-09-09 there was no identifier to match each event across the two datasets. Matching events after that is easy with player id, `OxfordStudyLocalTimeStamp`, and `OxfordStudyEventId`. But how to match events before that?

- For every player with data after 2022-09-09, find the time discrepancy between playfab and qualtrics timestamps by using `OxfordStudyLocalTimeStamp` and `TimeStamp` in playfab. Then adjust all older `TimeStamp`s in playfab accordingly, and match by player id, timestamp, and `event_name` (mood_reported / study_prompt_answered).
  - Even if the timezones are equalized, there will be a discrepancy of some seconds due to how the data is captured at playfab:

>Qualtric's timestamps are recorded on sending of the event from the participant's computer. Playfab's timestamps are recorded when the event has been received and stored in their database.
This means playfab's will be slightly later as it has to wait for both a) the time it takes for the information to be sent from the computer to Playfab's API (connection quality, upload speed, firewall etc can factor in here) and b) the time it takes for playfab to process and store the information in the right place  
  
- Use player's physical location info in every `player_logged_in` event to find their timezone, and adjust the playfab timestamps accordingly. Then match by timestamp.


We try some solutions below, which do not work.

Before having those data, we attempt to match by number of events by participant, and events' order. This does not work however.

```{r}
d <- inner_join(
  dq %>% 
    arrange(pid, timestamp) %>% 
    group_by(pid) %>% 
    # Ordering variable by person
    mutate(i = 1:n()) %>% 
    ungroup(),
  dp %>% 
    add_count(pid) %>% 
    # Drop ids who don't have a matching count in dq
    right_join(count(dq, pid)) %>% 
    select(-n) %>% 
    arrange(pid, timestamp) %>% 
    group_by(pid) %>% 
    mutate(i = 1:n()) %>% 
    ungroup() %>% 
    select(pid, i, timestamp_playfab = timestamp, event_name, everything()),
  by = c("pid", "i")
)
# I then check if the event names also matched -- for many rows they didn't, indicating this didn't work
d %>% 
  select(pid, starts_with("event_name"), starts_with("timesta"), response) %>% 
  filter(event_name.x != event_name.y)

# Then take out the nonmatching ones from rest of data
dq <- dq %>% 
  filter(!(pid %in% unique(d$pid)))
dp <- dp_survey %>% 
  filter(!(pid %in% unique(d$pid)))
```

We can also try to find the number of hours by which the two timestamps differ by comparing the modal response hour

```{r}
qualtrics_modes <- dq %>% 
  group_by(pid) %>% 
  mutate(
    hour = hour(timestamp)
  ) %>% 
  summarise(
    q_mode = which.max(tabulate((hour))),
    q_median = median(hour)
  )
distinct(dq, pid)

playfab_modes <- dp_survey %>% 
  group_by(pid) %>% 
  mutate(
    hour = hour(timestamp)
  ) %>% 
  summarise(
    p_mode = which.max(tabulate((hour))),
    p_median = median(hour)
  )
distinct(dp, pid)

# Adjust playfab timestamps by the difference in modal timestamps
dp_survey <- dp_survey %>% 
  left_join(full_join(qualtrics_modes, playfab_modes)) %>% 
  rename(timestamp_original = timestamp) %>% 
  mutate(timestamp = timestamp_original + (q_median-p_median)*3600)
```

We can then join the tables by player id and timestamp. When joining, we need to take into account that the playfab timestamps are also some seconds behind the qualtrics timestamps due to the way they are recorded. So we need to fuzzy join the data by some small temporal intervals.

Note when creating the intervals that qualtrics timestamp has to be earlier than corresponding playfab timestamp. Therefore the qualtrics interval starts at timestamp but ends later, and vice versa for playfab.

```{r}
library(fuzzyjoin)

# Interval joins ONLY join on the interval, so have to do pid by pid
tmp1 <- dq %>% 
  mutate(start = timestamp - 3, end = timestamp + 3) %>% 
  select(pid, timestamp, event_name, start, end) %>% 
  filter(pid %in% unique(.$pid)[1:10])
tmp2 <- dp_survey %>% 
  mutate(start = timestamp - 3, end = timestamp + 3) %>% 
  select(pid, timestamp, timestamp_original, event_name, start, end) %>% 
  filter(pid %in% unique(tmp1$pid))
unique(tmp1$pid) %>% 
  map(
    ~interval_left_join(
      filter(tmp1, pid == .x), 
      filter(tmp2, pid == .x) %>% drop_na(start, end),
      by = c("start", "end"),
      maxgap = 1
    )
  )
```

