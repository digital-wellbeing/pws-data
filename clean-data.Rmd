---
title: "Preprocess data"
output: html_document
date: "2022"
---

```{r setup, include=FALSE}
library(janitor)
library(knitr)
library(scales)
library(readxl)
library(lubridate)
library(DBI)
library(dtplyr)
library(dbplyr)
library(lutz)
library(tidyverse)
opts_chunk$set(
  message = FALSE
)
```

```{r}
# Load events x variables table from codebook
events <- read_excel(
  "data-raw/codebook.xlsx", 
  sheet = "Telemetry - Events & Variables"
) %>% 
  fill(Event) %>% 
  mutate(Event = snakecase::to_any_case(Event, "snake")) %>% 
  select(EventName = Event, Variables)
```

# Qualtrics

## Load data

```{r}
readRenviron(".env") # Read files from common OneDrive path
dq <- list.files(
  paste0(Sys.getenv("ONEDRIVE_PATH"), "data-raw/qualtrics/"),
  full.names = TRUE
) %>% 
  read_csv(
    col_types = cols_only(
      entityId = col_character(),
      timeStamp = col_character(),
      age = col_integer(),
      gender = col_character(),
      promptCategory = col_character(),
      response = col_double()  # Throws warnings for 'none' responses
    )
  )
# Verify that only problem is response='none'
problems() %>% 
  distinct(col, expected, actual)

# Remove test accounts
dq <- dq %>% 
  filter(
    !(entityId %in% c(
      "1b4893d8239b65417ea746c92db9c1adc686e8a1", 
      "47d03dc7c44a9e0a39ba3b5de3650b8f6e41a1d8", 
      "3086fcd6630a9c61d84884d56e49d34f87127b7d", 
      "785f413cb080eeac412538dc6424f240feb5b239", 
      "cc0458b60500702da2faf44e8af870795d7d9b52"))
  )

# Remove broken responses
dq <- dq %>% 
  drop_na(timeStamp)

# convert to datetimes here
dq <- dq %>% 
  mutate(timeStamp = as_datetime(timeStamp))
```

todo: what to do with people who have repeated age/gender responses? For now I am taking out people with more than two and keep the first responses.
```{r}
# Save demographics in a separate table
ids_qualtrics <- dq %>% 
  filter(is.na(promptCategory)) %>% 
  distinct(entityId, age, gender) %>% 
  add_count(entityId) %>% 
  filter(n <= 2) %>% 
  distinct(entityId, age, gender)
# Drop rows where demographics were reported
dq <- dq %>% 
  drop_na(promptCategory) %>% 
  select(-age, -gender)
# d_person %>% add_count(entityId) %>% filter(n > 1) %>% arrange(entityId) %>% view
```

### Remove faulty responses

The qualtrics data contains (massively) repeated responses; cases where for some reason the same response was being sent/recorded many times. Not only are these responses physically impossible, they will make the non-equi joins below impossible. These may also have occurred due to cached responses, which would not be found in the PlayFab data. For example:

```{r}
dq %>% 
  add_count(entityId, promptCategory, response, name = "repeats") %>% 
  filter(repeats == max(repeats))
```

We therefore apply a criterion to the Qualtrics data where subsequent responses from the same participant must be separated by at least one minute.

```{r}
dq <- dq %>% 
  arrange(entityId, timeStamp) %>% 
  lazy_dt() %>% 
  group_by(entityId) %>% 
  mutate(
    lag = difftime(timeStamp, lag(timeStamp), units = "secs")
  ) %>% 
  ungroup() %>% 
  collect()
dq %>% 
  filter(lag < 60 * 60) %>% 
  ggplot(aes(lag/60)) +
  scale_x_continuous(
    "Lag from previous response (minutes)",
    breaks = extended_breaks(9)
  ) +
  scale_y_continuous(expand = expansion(c(0, .1))) +
  geom_histogram(bins = 100)
dq %>% 
  mutate(lag_less = lag < 60) %>% 
  tabyl(lag_less)
dq <- dq %>% 
  filter(lag >= 60) %>% 
  select(-lag)
```

# PlayFab

```{r}
# Connect to local database
con <- dbConnect(
  RPostgres::Postgres(),
  dbname = "postgres",
  host = "localhost",
  port = 5432,
  user = "postgres",
  password = "postgres"
)
```

## IDs

We have three person identifier variables across two tables. Here, we match the IDs and keep only ones that exist in both data. We then create a new ID for all participants to add to both datasets.

```{r}
ids_playfab <- tbl(con, "pws") %>% 
  filter(EventName %in% c("study_prompt_answered", "mood_reported")) %>% 
  distinct(EntityId, OxfordStudyEntityId) %>% 
  collect() %>% 
  drop_na(OxfordStudyEntityId)
ids <- ids_playfab %>% 
  inner_join(
    distinct(ids_qualtrics, entityId), 
    by = join_by(
      x$OxfordStudyEntityId == y$entityId
    ),
    keep = TRUE
  ) %>% 
  mutate(pid = sprintf("%05d", 1:n()))

# Drop withdrawn participants'
ids <- anti_join(
  ids,
  tbl(con, "pws") %>% 
  filter(EventName == "clear_data_requested") %>% 
    distinct(EntityId) %>% 
    collect(),
)

# Replace all IDs with the new one
# Inner join ensures keeping only people with data in both tables
dq <- dq %>% 
  inner_join(distinct(ids, entityId, pid)) %>% 
  select(-entityId)
ids_qualtrics <- ids_qualtrics %>% 
  inner_join(distinct(ids, entityId, pid)) %>% 
  select(-entityId)
```

## Timezone offsets

We then find the timezone offsets to use in adjusting the playfab timestamps.

```{r}
# Find timezones
offsets <- tbl(con, "pws_device_info") %>% 
  select(
    EntityId,
    Timestamp,
    Longitude,
    Latitude
  ) %>% 
  distinct() %>% 
  collect() %>% 
  mutate(
    tz = tz_lookup_coords(Latitude, Longitude, method = "accurate")
  )

# In hits with multiple timezones, pick first
offsets <- offsets %>% 
  mutate(
    tz = if_else(
      str_detect(tz, ";"), 
      str_split_fixed(tz, ";", 2)[1],
      tz
    )
  )

# This is slow so do only for unique timezone-date pairs
tmp <- offsets %>% 
  mutate(date = ymd(str_sub(Timestamp, 1, 10))) %>%
  distinct(date, tz) %>%
  mutate(
    offset = map2_dbl(date, tz, ~tz_offset(.x, .y)$utc_offset_h)
  )
offsets <- offsets %>% 
  mutate(date = ymd(str_sub(Timestamp, 1, 10))) %>% 
  left_join(tmp) %>% 
  select(-c(date, Longitude, Latitude, tz))
rm(tmp)

# Use new pids
offsets <- offsets %>% 
  inner_join(distinct(ids, EntityId, pid)) %>% 
  relocate(pid, 1) %>% 
  select(-EntityId)
```

## Operations in database

We then process the playfab data in the database with dbplyr

```{r}
# Load data for valid participants
d <- tbl(con, "pws") %>% 
  filter(EventName == "mood_reported") %>% 
  inner_join(distinct(ids, EntityId, pid), copy = TRUE) %>% 
  # Keep only new pid
  select(-EntityId, -OxfordStudyEntityId)
  
# Keep only relevant events
d <- inner_join(d, distinct(events, EventName), copy = TRUE)

# Cleaning
d <- d %>% 
  # mood_reported events are always wellbeing reports
  mutate(
    LastStudyPromptType = ifelse(
      EventName == "mood_reported", 
      "Wellbeing", 
      LastStudyPromptType
    ),
    # The study_prompt_skipped event is actually study_prompt_answered where the response is missing
    EventName = ifelse(
      EventName == "study_prompt_skipped", 
      "study_prompt_answered", 
      EventName
    )
    # OxfordStudyLocalTimeStamp = as_datetime(OxfordStudyLocalTimeStamp)
  )
```

### Adjust timestamps

We then adjust the timestamps by appending the main table with the offset variable, then order by timestamp so that we can use the offsets for each following row (and preceding if no prior offsets existed).

todo: How can this be done in the database? Currently errors with:
```
Error: Failed to prepare query: ERROR:  operator does not exist: - timestamp with time zone
LINE 35: ...SE 1 END) OVER (ORDER BY -CAST("pid" AS NUMERIC), -"Timestam...
HINT:  No operator matches the given name and argument type. You might need to add an explicit type cast.
```

```{r}
# Append the offsets table to the main table
d <- rows_append(
  d %>% mutate(offset = NA),
  offsets %>% 
    mutate(EventName = "player_device_info"),
  copy = TRUE
)
d <- d %>%
  # Order by person and time so that offsets are filled in temporal order by person
  # todo: this doesn't work
  window_order(pid=as.numeric(pid), Timestamp) %>%
  fill(offset, .direction = "down") %>% 
  fill(offset, .direction = "up") %>% 
  ungroup()

# Create a new timestamp reflecting local time
d <- d %>% 
  mutate(timestamp = Timestamp + offset*3600) %>% 
  rename(timestamp_utc = Timestamp)

# We can then remove the `player_device_info` events.
d <- d %>% 
  filter(EventName != "player_device_info")
```

## Split to event type tables

Move to Duckdb tables by event name, saving only variables relevant to that event.

```{r}
library(duckdb)
duck <- dbConnect(
  duckdb(), 
  dbdir = "data-raw/playfab.duckdb", 
  read_only = FALSE
)
# These are no longer needed
removed_vars <- c("OxfordStudyEntityId", "EntityId", "Timestamp")
for (e in unique(events$EventName)) {
  print(e)
  x <- d %>% 
    # Pick this event
    filter(EventName == e) %>% 
    # And only variables collected at that event
    select(
      filter(
        events, 
        EventName == e, 
        !(Variables %in% removed_vars)
      ) %>% 
        pull(Variables)
    ) %>% 
    collect() %>% 
    # Convert character arrays to characters
    mutate(across(starts_with("Current"), as.character)) %>% 
    # Empty arrays to NA_character_
    mutate(across(starts_with("Current"), ~na_if(., "{}")))
  
  # Save to duckdb event table
  dbWriteTable(
    duck, e, x,
    overwrite = FALSE, 
    append = FALSE
  )
  
}
# dbDisconnect(duck, shutdown = TRUE)
```



## Telemetry

An early version of the game had a bug where `game_saved` events were being recorded too often. Here we re-apply the 10 second lower limit. 

```{r}
# Create a separate local table of game_saved events
# This is slower than doing in the database
d2 <- filter(d, EventName == "game_saved")
d <- anti_join(d, d2)
#d2 <- collect(d2)

# Create a lag with NAs for participants' first observations
d2 <- d2 %>% 
  arrange(EntityId, Timestamp) %>% 
  group_by(EntityId) %>% 
  mutate(Timestamp_lag = lag(Timestamp)) %>% 
  mutate(
    lag = sql('EXTRACT(EPOCH FROM ("Timestamp" - "Timestamp_lag"))')
  ) %>% 
  ungroup()

# Quick comparison to original R call
# TODO: this can be removed
lag_tmp <- d2 %>% 
  select(Timestamp, Timestamp_lag, lag) %>% 
  head(20) %>% 
  collect()

lag_tmp %>% 
  mutate(lag_R = difftime(Timestamp, lag(Timestamp), units = "secs"))

# Examine distribution of short lags
d2 %>% 
  filter(between(as.numeric(lag), 0, 25)) %>% 
  ggplot(aes(lag)) +
  geom_histogram(binwidth = .1) +
  labs(x = "lag (sec)") +
  scale_y_continuous(labels = comma)


# Take out observations with lag too short, allowing for some noise
d2 <- d2 %>% 
  filter(lag > 9.75)

# Join to main data and rearrange
d <- bind_rows(collect(d), d2) %>% 
  arrange(EntityId, Timestamp)
rm(d2)
```


# Join

## Harmonise names

Before joining the tables we create common names for variables.

```{r}
d <- clean_names(d) %>% 
  select(
    pid = oxford_study_entity_id,
    timestamp,
    timestamp_qts = oxford_study_local_time_stamp,
    timestamp_utc,
    event_name,
    prompt = last_study_prompt_type,
    order(colnames(d)),
    -c(multiplayer_mode, offset, oxford_study_event_id)
  )

dq <- dq %>% 
  rename(
    pid = entityId,
    timestamp_qts = timeStamp,
    prompt = promptCategory
  )
```

## Perform join

We then join the qualtrics survey data to the telemetry data. We treat the PlayFab telemetry as the primary table, because i. it contains the rest of the data (not just survey events), and ii. the Qualtrics survey data has large numbers of duplicated responses (some bug in sending data from PWS to the Qualtrics API), and treating it as secondary makes it easier to remove the duplicates automatically.

We work only on the survey event telemetry (because it is the only data that can join)

```{r}
dp <- d %>% 
  filter(event_name %in% c("mood_reported", "study_prompt_answered"))
d <- anti_join(d, dp) # Rest of playfab data
```

We join playfab (x) and qualtrics (y) first by joining with the common timestamp. That however doesn't exist in old data, for which we use the adjusted timestamps. That can also fail because of incorrectly configured clocks, VPNs, etc. So we join the remainder by unique prompt types and the mm:ss of the timestamps.

### 1. Join by common timestamp

Join on `timestamp_qts` when it exists in x. This ensures exact matches. This variable does not exist in oldest data and therefore doesn't work on all rows. We use `inner_join()` to keep matching rows only, and will use other methods below to deal with the dropped rows.

```{r}
dat1 <- inner_join(
  drop_na(dp, timestamp_qts),
  dq
)
```

Summarise matches and verify that no participant has more rows after joining (this would show as negative values of `dropped` below)

```{r}
full_join(
  count(dp, pid),
  count(dat1, pid, name = "n1")
) %>% 
  replace_na(list(n = 0, n1 = 0)) %>% 
  mutate(
    dropped = n-n1, 
    .keep = "used"
  ) %>% 
  summary
```

We then remove the already-joined rows from the tables

```{r}
dp <- anti_join(dp, dat1) %>% 
  # These are all empty due to above
  select(-timestamp_qts)
dq <- anti_join(dq, dat1)
```

### 2. Join by adjusted timestamp

Join remaining data on `timestamp` and `timestamp_qts` when the latter doesn't exist in x.

However, the two timestamps do not match to the millisecond because of different lags when data is sent from the game to playfab and qualtrics, and clock drift on the local machine. It is also possible that sometimes the timezone adjustment doesn't work because the geoip based location is wrong, or the player has adjusted their clock away from the actual local time.

We therefore do a non-equi join where the adjusted timestamp in x must be within some interval around the qualtrics timestamp for a row to match across x and y. 

Above, we removed rows from qualtrics where the response was within 60 seconds of the previous one. Therefore we use an interval that extends a maximum of |59|s to each side. Below, we see that lengthening the interval would not make a material difference anyway (but would include false alarms due to the 60-second rule).

```{r}
#' How many rows were dropped and added by an interval
#'
#' @param interval length of interval around qualtrics time in seconds
#'
#' 30% of the interval is allocated to before, and 70% after the qualtrics timestamp, because the qualtrics timestamp should usually be first
#'
#' @return tibble with number of dropped and added rows
test_join_2 <- function(interval = 119/2) {
  tmp <- inner_join(
    dp,
    dq %>% 
      mutate(
        a = timestamp_qts - seconds(interval * .5), 
        b = timestamp_qts + seconds(interval * .5)
      ),
    by = join_by(
      x$pid == y$pid,
      x$prompt == y$prompt,
      between(x$timestamp, y$a, y$b)
    )
  )
  full_join(
    count(dp, pid),
    count(tmp, pid, name = "n1"),
    by = "pid"
  ) %>% 
    replace_na(list(n = 0, n1 = 0)) %>% 
    mutate(dropped = n - n1) %>% 
    summarise(
      drops = sum(dropped[sign(dropped)==1]), 
      adds = abs(sum(dropped[sign(dropped)==-1]))
    )
}
tmp <- tibble(interval = seq(1, 241, by = 20)) %>% 
  mutate(x = map(interval, test_join_2)) %>% 
  unnest(x)
tmp
```

Based on this we pick a two-minute interval

```{r}
dat2 <- inner_join(
  dp,
  dq %>% 
    mutate(
      a = timestamp_qts - seconds(59.9), 
      b = timestamp_qts + seconds(59.9)
    ),
  by = join_by(
    x$pid == y$pid,
    x$prompt == y$prompt,
    between(x$timestamp, y$a, y$b)
  )
) %>% 
  select(-a, -b)
```

Summarise matches and verify that no participant has more rows after joining

```{r}
full_join(
  count(dp, pid),
  count(dat2, pid, name = "n1")
) %>% 
  replace_na(list(n = 0, n1 = 0)) %>% 
  mutate(
    dropped = n-n1, 
    .keep = "used"
  ) %>% 
  summary
```

Then stack the tables and take out the already-joined data from the rest of the data

```{r}
dat <- bind_rows(
  dat1, dat2
)

dp <- anti_join(dp, dat)
dq <- anti_join(dq, dat)
```

## Summary

Below, we summarise the results of the join. We could join a median of 94% of a participant's data, and are left with 109 participants for whom no joins were found.

```{r}
full_join(
  count(dat, pid),
  count(dp, pid, name = "n1")
) %>% 
  replace_na(list(n = 0, n1 = 0)) %>% 
  mutate(joined_p = n / (n+n1)) %>% 
  mutate(no_joins = n==0 & n1 > 0) %>% 
  summary
```

# Create final data

We replace the original participants IDs with other values

```{r}
d <- d %>% 
  left_join(
    distinct(d, pid) %>% 
      mutate(pid2 = sprintf("%05d", 1:n()))
  ) %>% 
  select(pid2, everything(), -pid) %>% 
  rename(pid = pid2)
```

Then save the data

```{r}
library(arrow)
dir.create("data/main", showWarnings = FALSE, recursive = TRUE)
d %>% 
  write_dataset(
    "data/main", 
    format = "arrow",
    partitioning = "event_name"
  )
```

