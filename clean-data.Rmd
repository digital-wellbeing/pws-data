---
title: "Preprocess data"
output: html_document
date: "2022"
---

```{r setup, include=FALSE}
library(janitor)
library(knitr)
library(scales)
library(readxl)
library(DBI)
library(data.table)
library(dtplyr)
library(dbplyr)
library(duckdb)
library(lutz)
library(lubridate)
library(tidyverse)
opts_chunk$set(
  message = FALSE,
  cache = TRUE,
  eval = !file.exists("data/data.duckdb")  # Don't run anything if already done
)
readRenviron(".env") # Read OneDrive path for original files
```

Local databases

```{r}
# Connect to local database with original PlayFab data
con <- dbConnect(
  RPostgres::Postgres(),
  dbname = "postgres",
  host = "localhost",
  port = 5432,
  user = "postgres",
  password = "postgres"
)

# Create a local database for processed output data
dir.create("data", FALSE)
duck <- dbConnect(
  duckdb(), 
  dbdir = "data/data.duckdb", 
  read_only = FALSE
)
```

```{r}
# Load events x variables table from codebook
events <- read_excel(
  paste0(Sys.getenv("ONEDRIVE_PATH"), "data-raw/codebook.xlsx"), 
  sheet = "Telemetry - Events & Variables"
) %>% 
  fill(Event) %>% 
  mutate(Event = snakecase::to_any_case(Event, "snake")) %>% 
  select(EventName = Event, Variables)
```

# Qualtrics

## Load data

```{r}
dq <- list.files(
  paste0(Sys.getenv("ONEDRIVE_PATH"), "data-raw/qualtrics/"),
  full.names = TRUE
) %>% 
  read_csv(
    col_types = cols_only(
      entityId = col_character(),
      timeStamp = col_character(),
      age = col_integer(),
      gender = col_character(),
      promptCategory = col_character(),
      response = col_double()  # Throws warnings for 'none' responses
    )
  )
# Verify that only problem is response='none'
problems() %>% 
  distinct(col, expected, actual)

# Remove test accounts
dq <- dq %>% 
  filter(
    !(entityId %in% c(
      "1b4893d8239b65417ea746c92db9c1adc686e8a1", 
      "47d03dc7c44a9e0a39ba3b5de3650b8f6e41a1d8", 
      "3086fcd6630a9c61d84884d56e49d34f87127b7d", 
      "785f413cb080eeac412538dc6424f240feb5b239", 
      "cc0458b60500702da2faf44e8af870795d7d9b52"))
  )

# Remove broken responses
dq <- dq %>% 
  drop_na(timeStamp)

# convert to datetimes here
dq <- dq %>% 
  mutate(timeStamp = as_datetime(timeStamp))
```

todo: what to do with people who have repeated age/gender responses? For now I am taking out people with more than two and keep the first responses.
```{r}
# Save demographics in a separate table
ids_qualtrics <- dq %>% 
  filter(is.na(promptCategory)) %>% 
  distinct(entityId, age, gender) %>% 
  add_count(entityId) %>% 
  filter(n <= 2) %>% 
  distinct(entityId, age, gender)
# Drop rows where demographics were reported
dq <- dq %>% 
  drop_na(promptCategory) %>% 
  select(-age, -gender)
# d_person %>% add_count(entityId) %>% filter(n > 1) %>% arrange(entityId) %>% view
```

## Remove faulty responses

The qualtrics data contains (massively) repeated responses; cases where for some reason the same response was being sent/recorded many times. Not only are these responses physically impossible, they will make the non-equi joins below impossible. These may also have occurred due to cached responses, which would not be found in the PlayFab data. For example:

```{r}
dq %>% 
  add_count(entityId, promptCategory, response, name = "repeats") %>% 
  filter(repeats == max(repeats))
```

We therefore apply a criterion to the Qualtrics data where subsequent responses from the same participant must be separated by at least one minute.

```{r}
dq <- dq %>% 
  arrange(entityId, timeStamp) %>% 
  lazy_dt() %>% 
  group_by(entityId) %>% 
  mutate(
    lag = difftime(timeStamp, lag(timeStamp), units = "secs")
  ) %>% 
  ungroup() %>% 
  collect()
dq %>% 
  filter(lag < 60 * 60) %>% 
  ggplot(aes(lag/60)) +
  scale_x_continuous(
    "Lag from previous response (minutes)",
    breaks = extended_breaks(9)
  ) +
  scale_y_continuous(expand = expansion(c(0, .1))) +
  geom_histogram(bins = 100)
dq %>% 
  mutate(lag_less = lag < 60) %>% 
  tabyl(lag_less)
dq <- dq %>% 
  filter(lag >= 60) %>% 
  select(-lag)
```

## Fix improperly configured clock data

There are some players in the data with improperly configured clocks such that the years are impossible. 

```{r}
dq %>% 
  count(
    year = year(timeStamp),
    month = month(timeStamp, label = TRUE)
  )
```

The year 1444 data doesn't have appropriate months as is therefore completely off, whereas the 2565 data can be fixed by just adjusting the year

```{r}
dq <- dq %>% 
  filter(year(timeStamp) >= 2022) %>% 
  mutate(
    timeStamp = if_else(
      year(timeStamp) >= 2565,
      timeStamp - years(2565 - 2022),
      timeStamp
      )
    )
```

This fixed it

```{r}
dq %>% 
  count(
    year = year(timeStamp),
    month = month(timeStamp, label = TRUE)
  )
```


# PlayFab

Next, we use dbplyr to process the data in the db.

## IDs

We have three person identifier variables across two tables. Here, we match the IDs and keep only ones that exist in both data. We then create a new ID for all participants to add to both datasets.

```{r}
ids_playfab <- tbl(con, "pws") %>% 
  filter(EventName %in% c("study_prompt_answered", "mood_reported")) %>% 
  distinct(EntityId, OxfordStudyEntityId) %>% 
  collect() %>% 
  drop_na(OxfordStudyEntityId)
ids <- ids_playfab %>% 
  inner_join(
    distinct(ids_qualtrics, entityId), 
    by = join_by(
      x$OxfordStudyEntityId == y$entityId
    ),
    keep = TRUE
  ) %>% 
  mutate(pid = sprintf("%05d", 1:n()))

# Drop withdrawn participants'
ids <- anti_join(
  ids,
  tbl(con, "pws") %>% 
  filter(EventName == "clear_data_requested") %>% 
    distinct(EntityId) %>% 
    collect(),
)

# Replace all IDs with the new one
# Inner join ensures keeping only people with data in both tables
dq <- dq %>% 
  inner_join(distinct(ids, entityId, pid)) %>% 
  select(-entityId)
ids_qualtrics <- ids_qualtrics %>% 
  inner_join(distinct(ids, entityId, pid)) %>% 
  select(-entityId)
```

## Timezone offsets

We then find the timezone offsets to use in adjusting the playfab timestamps.

```{r}
# Find timezones
offsets <- tbl(con, "pws_device_info") %>% 
  select(
    EntityId,
    Timestamp,
    Longitude,
    Latitude
  ) %>% 
  distinct() %>% 
  collect() %>% 
  mutate(
    tz = tz_lookup_coords(Latitude, Longitude, method = "accurate")
  )

# In hits with multiple timezones, pick first
offsets <- offsets %>% 
  mutate(
    tz = if_else(
      str_detect(tz, ";"), 
      str_split_fixed(tz, ";", 2)[1],
      tz
    )
  )

# This is slow so do only for unique timezone-date pairs
tmp <- offsets %>% 
  mutate(date = ymd(str_sub(Timestamp, 1, 10))) %>%
  distinct(date, tz) %>%
  mutate(
    offset = map2_dbl(date, tz, ~tz_offset(.x, .y)$utc_offset_h)
  )
offsets <- offsets %>% 
  mutate(date = ymd(str_sub(Timestamp, 1, 10))) %>% 
  left_join(tmp) %>% 
  select(-c(date, Longitude, Latitude, tz))
rm(tmp)

# Use new pids
offsets <- offsets %>% 
  inner_join(distinct(ids, EntityId, pid)) %>% 
  relocate(pid, 1) %>% 
  select(-EntityId)

# We only need the offset for when it changed
offsets <- distinct(offsets, pid, offset, .keep_all = TRUE)
```

## Load data and clean

We then process the playfab data in the database with dbplyr

```{r}
# Load data for valid participants
d <- tbl(con, "pws") %>% 
  inner_join(distinct(ids, EntityId, pid), copy = TRUE) %>% 
  # Keep only new pid
  select(-EntityId, -OxfordStudyEntityId)
  
# Keep only relevant events
d <- inner_join(d, distinct(events, EventName), copy = TRUE)

# Cleaning
d <- d %>% 
  mutate(
    # study_prompt_skipped is study_prompt_answered where response is missing
    EventName = ifelse(
      EventName == "study_prompt_skipped", 
      "study_prompt_answered", 
      EventName
    )
  )
```

# Save tables

Next we write the tables to a local DuckDB database by event type. I use the events & variables pairing from the codebook to save appropriate variables with each event. 

While the tables are in R we do two important operations:

- adjust the timestamps by appending the main table with the offset variable, then order by timestamp so that we can use the offsets for each following row (and preceding if no prior offsets existed). This allows adjusting timestamps with variable offsets (e.g. person moves back and forth).
- fix a bug with the game_saved events here that caused them to be sent too often.

```{r}
# An early version of the game had a bug where `game_saved` events were being recorded too often. This function re-applies the 10 second lower limit. 
fix_game_saved_bug <- function(data) {
  data %>% 
    lazy_dt() %>% 
    # Create a lag with NAs for participants' first observations
    arrange(pid, Time) %>%
    group_by(pid) %>% 
    mutate(Time_lag = lag(Time)) %>% 
    mutate(
      lag = difftime(Time, Time_lag, units = "secs")
    ) %>% 
    ungroup() %>% 
    # Take out observations with lag too short, allowing for some noise
    # Do not take out rows where lag is missing (first observations)
    filter(lag > 9.95 | is.na(lag)) %>% 
    select(-lag) %>% 
    collect()
}
```

```{r}
# Add common events for internal use (joining)
events <- events %>% 
  bind_rows(
    tibble(
      EventName = c("study_prompt_answered", "mood_reported"),
      Variables = "OxfordStudyLocalTimeStamp"
    )
  ) %>% 
  arrange(EventName)

adjust_timestamp_and_save <- function(e) {
  
  message("Saving ", e)
  
  # Save only the variables relevant to this event
  vars <- events[events$EventName == e, "Variables", drop = TRUE]
  
  # Pull table to R
  x <- d %>% 
    # Pick this event
    filter(EventName == e) %>% 
    # And only variables collected at that event + pid + EventName
    select(
      pid,
      EventName,
      any_of(vars),
    ) %>% 
    lazy_dt() %>% 
    # Convert character arrays to characters & empty arrays to NAs
    mutate(across(starts_with("Current"), ~na_if(as.character(.), "{}"))) %>% 
    # There are duplicates of some rows, take out
    distinct()
  
  # These offsets
  y <- offsets %>% 
    mutate(EventName = "player_device_info") %>% 
    lazy_dt()
  
  # Adjust timezones
  # Stack with timezone offsets
  x <- rbind(
    as.data.table(x), 
    as.data.table(y), 
    fill = TRUE
  ) %>% 
    lazy_dt() %>% 
    # Fill offset appropriately to each row
    arrange(pid, Timestamp) %>% 
    group_by(pid) %>% 
    fill(offset, .direction = "downup") %>%
    # Create new local timestamp and rename old
    rename(Time_utc = Timestamp) %>% 
    mutate(
      Time = Time_utc + offset*3600, 
    ) %>% 
    # Remove the `player_device_info` events and offsets
    filter(
      EventName != "player_device_info" | is.na(EventName)
    ) %>% 
    select(-offset) %>% 
    relocate(pid, EventName, Time, Time_utc) %>% 
    collect()
  
  if (e == "game_saved") {
    x <- fix_game_saved_bug(x)
  }
  
  # Save to duckdb event table
  dbWriteTable(
    duck, e, x,
    overwrite = TRUE, 
    append = FALSE
  )
  
}

walk(unique(events$EventName), adjust_timestamp_and_save)

dbDisconnect(con)
rm(d, con, offsets)
```


# Join survey data

We then join the qualtrics survey data to the telemetry data. We treat the PlayFab telemetry as the primary table, because i. it contains the rest of the data (not just survey events), and ii. the Qualtrics survey data has large numbers of duplicated responses (some bug in sending data from PWS to the Qualtrics API), and treating it as secondary makes it easier to remove the duplicates automatically.

We work only on the survey event telemetry (because it is the only data that can join)

```{r}
playfab_survey_data <- bind_rows(
  tbl(duck, "mood_reported") %>% 
    collect() %>% 
    # These are always wellbeing questions
    mutate(LastStudyPromptType = "Wellbeing"),
  tbl(duck, "study_prompt_answered") %>% 
    collect()
) %>% 
  mutate(OxfordStudyLocalTimeStamp = as_datetime(OxfordStudyLocalTimeStamp))
```

We join playfab (x) and qualtrics (y) first by joining with the common timestamp. That however doesn't exist in old data, for which we use the adjusted timestamps. That can also fail because of incorrectly configured clocks, VPNs, etc. So we join the remainder by unique prompt types and the mm:ss of the timestamps.

### 1. Join by common timestamp

Join on `timestamp_qts` when it exists in x. This ensures exact matches. This variable does not exist in oldest data and therefore doesn't work on all rows. We use `inner_join()` to keep matching rows only, and will use other methods below to deal with the dropped rows.

```{r}
playfab_survey_data_joined <- inner_join(
  drop_na(playfab_survey_data, OxfordStudyLocalTimeStamp),
  dq,
  by = join_by(
    x$pid == y$pid,
    x$LastStudyPromptType == y$promptCategory,
    x$OxfordStudyLocalTimeStamp == y$timeStamp
    )
)
```

### 2. Join by adjusted timestamp

Join remaining data on adjusted playfab timestamp and qualtrics timestamp.

However, the two timestamps do not match to the millisecond because of different lags when data is sent from the game to playfab and qualtrics, and clock drift on the local machine. It is also possible that sometimes the timezone adjustment doesn't work because the geoip based location is wrong, or the player has adjusted their clock away from the actual local time.

We therefore do a non-equi join where the adjusted PF timestamp in x must be within some interval around the qualtrics timestamp for a row to match across x and y. 

Above, we removed rows from qualtrics where the response was within 60 seconds of the previous one. Therefore we use an interval that extends a maximum of |59|s to each side. 

```{r}
# Join data that isn't already joined
playfab_survey_data_joined <- inner_join(
  # Playfab data - already joined
  anti_join(
    playfab_survey_data, 
    playfab_survey_data_joined
  ),
  # Qualtrics data - already joined
  anti_join(
    dq,
    playfab_survey_data_joined,
    by = join_by(
      pid,
      x$promptCategory == y$LastStudyPromptType,
      x$timeStamp == y$OxfordStudyLocalTimeStamp
    )
  ) %>% 
    mutate(
      a = timeStamp - seconds(59.99), 
      b = timeStamp + seconds(59.99)
    ),
  # Join by PID, prompt type, and playfab timestamp within qualtrics interval
  by = join_by(
    pid,
    x$LastStudyPromptType == y$promptCategory,
    between(x$Time, y$a, y$b)
  )
) %>% 
  select(-a, -b) %>% 
  # Stack with already-joined data
  bind_rows(
    playfab_survey_data_joined
  )
```

### Summary

Below, we summarise the results of the join. `joined_p` are summarise of the per-participants proportions of telemetry events for which survey data was found.

```{r}
join_summary <- full_join(
  count(playfab_survey_data, pid, name = "n_original"),
  count(playfab_survey_data_joined, pid, name = "n_joined")
) %>% 
  replace_na(list(n_original = 0, n_joined = 0)) %>% 
  mutate(joined_p = n_joined / (n_original))
join_summary %>%   
  ggplot(aes(joined_p)) +
  scale_y_continuous(
    "Number of participants",
    expand = expansion(c(0, .1))
  ) +
  xlab("Proportion of playfab rows joined") +
  geom_histogram(bins = 100)

# Odd spikes indicate many people with very few observations
join_summary %>% 
  ggplot(aes(n_original, n_joined, col = joined_p %in% c(0, 0.5))) +
  geom_abline(slope = 1, size = .33, lty = 2) +
  xlab(
    "Number of survey rows in original playfab data"
  ) +
  ylab(
    "Number of rows joined"
  ) +
  geom_point(size = 1, alpha = .75) +
  theme(aspect.ratio = 1, legend.position = "bottom")
```

In total this is how much data is lost

```{r}
# People lost (but see above they are mostly people who started with n=2 or so)
length(unique(playfab_survey_data$pid)) - length(unique(playfab_survey_data_joined$pid))

# Total rows lost
nrow(playfab_survey_data) - nrow(playfab_survey_data_joined)

# This percentage is small
(nrow(playfab_survey_data) - nrow(playfab_survey_data_joined)) / nrow(playfab_survey_data)
```


### Save

```{r}
# Overwrite tables in duckdb
playfab_survey_data_joined %>% 
  select(-OxfordStudyLocalTimeStamp, -timeStamp) %>% 
  group_by(EventName) %>% 
  group_walk(
    ~dbWriteTable(
      duck, as.character(.y), .x,
      overwrite = TRUE, 
      append = FALSE
    ),
    .keep = TRUE
  )
```

# Summary

The cleaned data with new IDs is now written in a local DuckDB database. It is split to tables per event, with each table only containing variables relevant to that event.

Here are the number of rows in each table

```{r}
tmp <- events %>% 
  distinct(EventName) %>% 
  mutate(
    rows = map_dbl(
      EventName, 
      ~tbl(duck, .x) %>% 
        tally() %>% 
        collect() %>% 
        pull(n)
      )
  )
tmp %>% 
  mutate(rows = number(rows, big.mark = ","))
```

These tables are easily accessible

```{r}
tbl(duck, "mood_reported") %>% glimpse()
tbl(duck, "player_logged_in") %>% glimpse()
```

We have also saved them as compressed comma-separated tables

```{r}
unique(events$EventName) %>% 
  walk(
    ~tbl(duck, .x) %>% 
      collect() %>% 
      write_csv(str_glue("data/{.x}.csv.gz"))
  )

read_csv("data/mood_reported.csv.gz") %>% head()
```

An additional table includes basic demographics

```{r}
ids_qualtrics <- ids_qualtrics %>% 
  arrange(pid) %>% 
  relocate(pid)

dbWriteTable(
  duck, "demographics", ids_qualtrics,
  overwrite = TRUE, 
  append = FALSE
)
write_csv(ids_qualtrics, "data/demographics.csv.gz")
```

```{r}
#| include: false
dbDisconnect(duck, shutdown=TRUE)
```

