---
title: "Preprocess data"
output: html_document
date: "2022"
---

```{r setup, include=FALSE}
library(janitor)
library(knitr)
library(scales)
library(lubridate)
library(DBI)
library(dbplyr)
library(lutz)
library(tidyverse)
opts_chunk$set(
  message = FALSE
)
```

# PlayFab

## Load data

```{r}
# Connect to local database
con <- dbConnect(
  RPostgres::Postgres(),
  dbname = "postgres",
  host = "localhost",
  port = 5432,
  user = "postgres",
  password = "postgres"
)
# Exclude some events
events <- c(
  # Not used
  "player_inventory_item_added",
  "player_consumed_item"
  
)
d <- tbl(con, "pws") %>% 
  filter(!(EventName %in% events)) %>% 
  collect()
```


## Remove unnecessary data

### Clear data requested

Completely remove data from people who requested it

```{r}
d <- anti_join(
  d,
  d %>% 
    filter(EventName == "clear_data_requested") %>% 
    distinct(EntityId)
)
```

### Event types

We only care about player events

```{r}
d <- d %>% 
  filter(
    EntityType == "player",
    !(EventName %in% c(
      "player_added_title", 
      "player_created", 
      "player_linked_account"
    ))
  ) %>% 
  select(-EntityType)
```

### Variable types

Convert character arrays to characters

```{r}
d <- d %>% 
  mutate(across(starts_with("Current"), as.character)) %>% 
  # Empty arrays to NA_character_
  mutate(across(starts_with("Current"), ~na_if(., "{}")))
```

### Telemetry

An early version of the game had a bug where `game_saved` events were being recorded too often. Here we re-apply the 10 second lower limit. todo


## Adjust timestamps

We first find timezone offsets

```{r}
# Find timezones
d_device <- tbl(con, "pws_device_info") %>% 
  collect() %>% 
  distinct()
offsets <- d_device %>% 
  select(
    EntityId,
    Timestamp,
    Longitude,
    Latitude
  ) %>% 
  mutate(
    tz = tz_lookup_coords(Latitude, Longitude, method = "accurate")
  )

# In hits with multiple timezones, pick first
offsets <- offsets %>% 
  mutate(
    tz = if_else(
      str_detect(tz, ";"), 
      str_split_fixed(tz, ";", 2)[1],
      tz
    )
  )

# This is slow so do only for unique timezone-date pairs
tmp <- offsets %>% 
  mutate(date = ymd(str_sub(Timestamp, 1, 10))) %>%
  distinct(date, tz) %>%
  mutate(
    offset = map2_dbl(date, tz, ~tz_offset(.x, .y)$utc_offset_h)
  )
offsets <- offsets %>% 
  mutate(date = ymd(str_sub(Timestamp, 1, 10))) %>% 
  left_join(tmp) %>% 
  select(-c(date, Longitude, Latitude, tz))
rm(tmp)
```

Then join those offsets to the main data

```{r}
d <- bind_rows(
  d,
  offsets %>% 
    mutate(EventName = "player_device_info")
) %>% 
  arrange(EntityId, Timestamp)
rm(offsets)
```

Fill offsets to player's following events, until the next offset. Then do it again but fill up to fill any remaining rows.

```{r}
d <- d %>% 
  group_by(EntityId) %>% 
  fill(offset, .direction = "downup") %>% 
  ungroup()
```

Create a new timestamp reflecting local time

```{r}
d <- d %>% 
  mutate(timestamp = Timestamp + offset*3600) %>% 
  rename(timestamp_utc = Timestamp)
```

We can then remove the `player_device_info` events.

```{r}
d <- d %>% 
  filter(EventName != "player_device_info")
```

## Cleaning

### Person IDs

We have two person identifiers `EntityId` (exists on all rows) and `OxfordStudyEntityId` (doesn't exist on all rows). Only the latter exists in the survey data, so we fill it to every row, and remove the unnecessary identifier.

```{r}
d <- d %>% 
  group_by(EntityId) %>% 
  fill(OxfordStudyEntityId, .direction = "downup") %>% 
  ungroup() %>% 
  select(-EntityId)
```

### Events

The study_prompt_skipped event is actually study_prompt_answered where the response is missing

```{r}
d <- d %>% 
  mutate(
    EventName = if_else(
      EventName == "study_prompt_skipped", 
      "study_prompt_answered", 
      EventName
    )
  )
```

mood_reported events are always wellbeing reports

```{r}
distinct(d, EventName, LastStudyPromptType)
d <- d %>% 
  mutate(
    LastStudyPromptType = if_else(
      EventName == "mood_reported", 
      "Wellbeing", 
      LastStudyPromptType
    )
  )
```

### Variable types

```{r}
d <- d %>% 
  mutate(OxfordStudyLocalTimeStamp = as_datetime(OxfordStudyLocalTimeStamp))
```

### Duplicated rows

Some rows are duplicated; we only keep unique rows

```{r}
d <- distinct(d)
```

# Qualtrics

## Load data

```{r}
readRenviron(".env") # Read files from common OneDrive path
dq <- list.files(
  paste0(Sys.getenv("ONEDRIVE_PATH"), "data-raw/qualtrics/"),
  full.names = TRUE
) %>% 
  read_csv(
    col_types = cols_only(
      entityId = col_character(),
      timeStamp = col_character(),
      age = col_integer(),
      gender = col_character(),
      promptCategory = col_character(),
      response = col_double()  # Throws warnings for 'none' responses
    )
  )
# Verify that only problem is response='none'
problems() %>% 
  distinct(col, expected, actual)
```

## Cleaning

```{r}
# Save demographics in a separate table
d_person <- dq %>% 
  filter(is.na(promptCategory)) %>% 
  distinct(entityId, age, gender)
# Drop rows where demographics were reported
dq <- dq %>% 
  drop_na(promptCategory) %>% 
  select(-age, -gender)

```

Timestamps are read as strings or it errs on many rows, convert to datetimes here.

```{r}
dq <- dq %>% 
  mutate(timeStamp = as_datetime(timeStamp))
```

There are data without a timestamp -- these are faulty responses between game and APIs. We remove those here.

```{r}
dq <- dq %>% 
  drop_na(timeStamp)
```

We then remove test accounts' data, these were for internal testing purposes

```{r}
dq <- dq %>% 
  filter(
    !(entityId %in% c(
      "1b4893d8239b65417ea746c92db9c1adc686e8a1", 
      "47d03dc7c44a9e0a39ba3b5de3650b8f6e41a1d8", 
      "3086fcd6630a9c61d84884d56e49d34f87127b7d", 
      "785f413cb080eeac412538dc6424f240feb5b239", 
      "cc0458b60500702da2faf44e8af870795d7d9b52"))
  )
```

### Remove faulty responses

The qualtrics data contains (massively) repeated responses; cases where for some reason the same response was being sent/recorded many times. Not only are these responses physically impossible, they will make the non-equi joins below impossible. These may also have occurred due to cached responses, which would not be found in the PlayFab data. For example:

```{r}
dq %>% 
  add_count(entityId, promptCategory, response, name = "repeats") %>% 
  filter(repeats == max(repeats))
```

We therefore apply a criterion to the Qualtrics data where subsequent responses from the same participant must be separated by at least one minute.

```{r}
dq <- arrange(dq, entityId, timeStamp)
dq <- dq %>% 
  group_by(entityId) %>% 
  mutate(
    lag = difftime(timeStamp, lag(timeStamp), units = "secs")
  ) %>% 
  ungroup()
dq %>% 
  filter(lag < 60 * 60) %>% 
  ggplot(aes(lag/60)) +
  scale_x_continuous(
    "Lag from previous response (minutes)",
    breaks = extended_breaks(9)
  ) +
  geom_histogram(bins = 100)
dq %>% 
  mutate(lag_less = lag < 60) %>% 
  tabyl(lag_less)
dq <- dq %>% 
  filter(lag >= 60) %>% 
  select(-lag)
```


# Join

## Harmonise names

Before joining the tables we create common names for variables.

```{r}
d <- clean_names(d) %>% 
  select(
    pid = oxford_study_entity_id,
    timestamp,
    timestamp_qts = oxford_study_local_time_stamp,
    timestamp_utc,
    event_name,
    prompt = last_study_prompt_type,
    order(colnames(d)),
    -c(multiplayer_mode, offset, oxford_study_event_id)
  )

dq <- dq %>% 
  rename(
    pid = entityId,
    timestamp_qts = timeStamp,
    prompt = promptCategory
  )
```

## Define participant

In order for the data to be meaningful, participants must have data in both tables

```{r}
before <- length(unique(d$pid))
d <- inner_join(
  d,
  distinct(dq, pid)
)
after <- length(unique(d$pid))
```

We removed `r before-after` players with telemetry only.

## Perform join

We then join the qualtrics survey data to the telemetry data. We treat the PlayFab telemetry as the primary table, because i. it contains the rest of the data (not just survey events), and ii. the Qualtrics survey data has large numbers of duplicated responses (some bug in sending data from PWS to the Qualtrics API), and treating it as secondary makes it easier to remove the duplicates automatically.

We work only on the survey event telemetry (because it is the only data that can join)

```{r}
dp <- d %>% 
  filter(event_name %in% c("mood_reported", "study_prompt_answered"))
d <- anti_join(d, dp) # Rest of playfab data
```

We join playfab (x) and qualtrics (y) first by joining with the common timestamp. That however doesn't exist in old data, for which we use the adjusted timestamps. That can also fail because of incorrectly configured clocks, VPNs, etc. So we join the remainder by unique prompt types and the mm:ss of the timestamps.

### 1. Join by common timestamp

Join on `timestamp_qts` when it exists in x. This ensures exact matches. This variable does not exist in oldest data and therefore doesn't work on all rows. We use `inner_join()` to keep matching rows only, and will use other methods below to deal with the dropped rows.

```{r}
dat1 <- inner_join(
  drop_na(dp, timestamp_qts),
  dq
)
```

Summarise matches and verify that no participant has more rows after joining (this would show as negative values of `dropped` below)

```{r}
full_join(
  count(dp, pid),
  count(dat1, pid, name = "n1")
) %>% 
  replace_na(list(n = 0, n1 = 0)) %>% 
  mutate(
    dropped = n-n1, 
    .keep = "used"
  ) %>% 
  summary
```

We then remove the already-joined rows from the tables

```{r}
dp <- anti_join(dp, dat1) %>% 
  # These are all empty due to above
  select(-timestamp_qts)
dq <- anti_join(dq, dat1)
```

### 2. Join by adjusted timestamp

Join remaining data on `timestamp` and `timestamp_qts` when the latter doesn't exist in x.

However, the two timestamps do not match to the millisecond because of different lags when data is sent from the game to playfab and qualtrics, and clock drift on the local machine. It is also possible that sometimes the timezone adjustment doesn't work because the geoip based location is wrong, or the player has adjusted their clock away from the actual local time.

We therefore do a non-equi join where the adjusted timestamp in x must be within some interval around the qualtrics timestamp for a row to match across x and y. 

Above, we removed rows from qualtrics where the response was within 60 seconds of the previous one. Therefore we use an interval that extends a maximum of |59|s to each side. Below, we see that lengthening the interval would not make a material difference anyway (but would include false alarms due to the 60-second rule).

```{r}
#' How many rows were dropped and added by an interval
#'
#' @param interval length of interval around qualtrics time in seconds
#'
#' 30% of the interval is allocated to before, and 70% after the qualtrics timestamp, because the qualtrics timestamp should usually be first
#'
#' @return tibble with number of dropped and added rows
test_join_2 <- function(interval = 119/2) {
  tmp <- inner_join(
    dp,
    dq %>% 
      mutate(
        a = timestamp_qts - seconds(interval * .5), 
        b = timestamp_qts + seconds(interval * .5)
      ),
    by = join_by(
      x$pid == y$pid,
      x$prompt == y$prompt,
      between(x$timestamp, y$a, y$b)
    )
  )
  full_join(
    count(dp, pid),
    count(tmp, pid, name = "n1"),
    by = "pid"
  ) %>% 
    replace_na(list(n = 0, n1 = 0)) %>% 
    mutate(dropped = n - n1) %>% 
    summarise(
      drops = sum(dropped[sign(dropped)==1]), 
      adds = abs(sum(dropped[sign(dropped)==-1]))
    )
}
tmp <- tibble(interval = seq(1, 241, by = 20)) %>% 
  mutate(x = map(interval, test_join_2)) %>% 
  unnest(x)
tmp
```

Based on this we pick a two-minute interval

```{r}
dat2 <- inner_join(
  dp,
  dq %>% 
    mutate(
      a = timestamp_qts - seconds(59.9), 
      b = timestamp_qts + seconds(59.9)
    ),
  by = join_by(
    x$pid == y$pid,
    x$prompt == y$prompt,
    between(x$timestamp, y$a, y$b)
  )
) %>% 
  select(-a, -b)
```

Summarise matches and verify that no participant has more rows after joining

```{r}
full_join(
  count(dp, pid),
  count(dat2, pid, name = "n1")
) %>% 
  replace_na(list(n = 0, n1 = 0)) %>% 
  mutate(
    dropped = n-n1, 
    .keep = "used"
  ) %>% 
  summary
```

Then stack the tables and take out the already-joined data from the rest of the data

```{r}
dat <- bind_rows(
  dat1, dat2
)

dp <- anti_join(dp, dat)
dq <- anti_join(dq, dat)
```

## Summary

Below, we summarise the results of the join. We could join a median of 94% of a participant's data, and are left with 109 participants for whom no joins were found.

```{r}
full_join(
  count(dat, pid),
  count(dp, pid, name = "n1")
) %>% 
  replace_na(list(n = 0, n1 = 0)) %>% 
  mutate(joined_p = n / (n+n1)) %>% 
  mutate(no_joins = n==0 & n1 > 0) %>% 
  summary
```

# Create final data

We replace the original participants IDs with other values

```{r}
set.seed(9845)
new_pids <- do.call(expand.grid, rep(list(c(letters, LETTERS)), 3)) %>% 
  mutate(id = paste0(Var1, Var2, Var3), .keep = "none") %>% 
  pull() %>% sample(size = length(unique(d$pid)))
new_pids <- distinct(d, pid) %>% 
  mutate(new_pid = new_pids)
d <- d %>% 
  left_join(new_pids) %>% 
  select(new_pid, everything(), -pid) %>% 
  rename(pid = new_pid)
```

Then save the data

```{r}
library(arrow)
dir.create("data/main", showWarnings = FALSE, recursive = TRUE)
d %>% 
  write_dataset(
    "data/main", 
    format = "parquet",
    partitioning = "event_name"
  )
```

